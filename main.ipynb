{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRdqZpx--Ycs"
      },
      "source": [
        "This is the main code for the paper titled **\"Improving Out-of-Distribution Data Handling and Corruption Resistance via Modern Hopfield Networks\"**."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A5hKUCUBVPDd",
        "outputId": "50593898-c694-4228-c982-0ab9abd5f27b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/Hopfield/')"
      ],
      "metadata": {
        "id": "omVE2w0SVXgM"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip3 install torchgeo"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "4NK1YqXQq2n9",
        "outputId": "d4855afa-2ddc-4423-a5c4-2c188074882c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torchgeo\n",
            "  Downloading torchgeo-0.7.0-py3-none-any.whl.metadata (19 kB)\n",
            "Requirement already satisfied: einops>=0.3 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (0.8.1)\n",
            "Collecting fiona>=1.8.22 (from torchgeo)\n",
            "  Downloading fiona-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (56 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/56.6 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.6/56.6 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kornia>=0.7.4 (from torchgeo)\n",
            "  Downloading kornia-0.8.1-py2.py3-none-any.whl.metadata (17 kB)\n",
            "Collecting lightly!=1.4.26,>=1.4.5 (from torchgeo)\n",
            "  Downloading lightly-1.5.21-py3-none-any.whl.metadata (37 kB)\n",
            "Collecting lightning!=2.3.*,!=2.5.0,>=2 (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo)\n",
            "  Downloading lightning-2.5.1.post0-py3-none-any.whl.metadata (39 kB)\n",
            "Requirement already satisfied: matplotlib>=3.6 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (3.10.0)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (2.0.2)\n",
            "Requirement already satisfied: pandas>=1.5 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (2.2.2)\n",
            "Requirement already satisfied: pillow>=9.2 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (11.2.1)\n",
            "Requirement already satisfied: pyproj>=3.4 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (3.7.1)\n",
            "Collecting rasterio!=1.4.0,!=1.4.1,!=1.4.2,>=1.3.3 (from torchgeo)\n",
            "  Downloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (9.1 kB)\n",
            "Collecting rtree>=1.0.1 (from torchgeo)\n",
            "  Downloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.1 kB)\n",
            "Collecting segmentation-models-pytorch>=0.3.3 (from torchgeo)\n",
            "  Downloading segmentation_models_pytorch-0.5.0-py3-none-any.whl.metadata (17 kB)\n",
            "Requirement already satisfied: shapely>=1.8.5 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (2.1.1)\n",
            "Requirement already satisfied: timm>=0.9.2 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (1.0.15)\n",
            "Requirement already satisfied: torch>=2 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (2.6.0+cu124)\n",
            "Collecting torchmetrics>=1.2 (from torchgeo)\n",
            "  Downloading torchmetrics-1.7.3-py3-none-any.whl.metadata (21 kB)\n",
            "Requirement already satisfied: torchvision>=0.15.1 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (0.21.0+cu124)\n",
            "Requirement already satisfied: typing-extensions>=4.5 in /usr/local/lib/python3.11/dist-packages (from torchgeo) (4.14.0)\n",
            "Requirement already satisfied: attrs>=19.2.0 in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.22->torchgeo) (25.3.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.22->torchgeo) (2025.4.26)\n",
            "Requirement already satisfied: click~=8.0 in /usr/local/lib/python3.11/dist-packages (from fiona>=1.8.22->torchgeo) (8.2.1)\n",
            "Collecting click-plugins>=1.0 (from fiona>=1.8.22->torchgeo)\n",
            "  Downloading click_plugins-1.1.1-py2.py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting cligj>=0.5 (from fiona>=1.8.22->torchgeo)\n",
            "  Downloading cligj-0.7.2-py3-none-any.whl.metadata (5.0 kB)\n",
            "Collecting kornia_rs>=0.1.9 (from kornia>=0.7.4->torchgeo)\n",
            "  Downloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from kornia>=0.7.4->torchgeo) (24.2)\n",
            "Collecting hydra-core>=1.0.0 (from lightly!=1.4.26,>=1.4.5->torchgeo)\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl.metadata (5.5 kB)\n",
            "Collecting lightly_utils~=0.0.0 (from lightly!=1.4.26,>=1.4.5->torchgeo)\n",
            "  Downloading lightly_utils-0.0.2-py3-none-any.whl.metadata (1.4 kB)\n",
            "Requirement already satisfied: python_dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.9.0.post0)\n",
            "Requirement already satisfied: requests>=2.27.0 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.32.3)\n",
            "Requirement already satisfied: six>=1.10 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (1.17.0)\n",
            "Requirement already satisfied: tqdm>=4.44 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (4.67.1)\n",
            "Requirement already satisfied: pydantic>=1.10.5 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.11.5)\n",
            "Collecting pytorch_lightning>=1.0.4 (from lightly!=1.4.26,>=1.4.5->torchgeo)\n",
            "  Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl.metadata (20 kB)\n",
            "Requirement already satisfied: urllib3>=1.25.3 in /usr/local/lib/python3.11/dist-packages (from lightly!=1.4.26,>=1.4.5->torchgeo) (2.4.0)\n",
            "Collecting aenum>=3.1.11 (from lightly!=1.4.26,>=1.4.5->torchgeo)\n",
            "  Downloading aenum-3.1.16-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: PyYAML<8.0,>=5.4 in /usr/local/lib/python3.11/dist-packages (from lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (6.0.2)\n",
            "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (2025.3.2)\n",
            "Collecting lightning-utilities<2.0,>=0.10.0 (from lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo)\n",
            "  Downloading lightning_utilities-0.14.3-py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting jsonargparse<5.0,>=4.27.7 (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo)\n",
            "  Downloading jsonargparse-4.40.0-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: omegaconf<3.0,>=2.2.3 in /usr/local/lib/python3.11/dist-packages (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (2.3.0)\n",
            "Requirement already satisfied: rich<14.0,>=12.3.0 in /usr/local/lib/python3.11/dist-packages (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (13.9.4)\n",
            "Collecting tensorboardX<3.0,>=2.2 (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo)\n",
            "  Downloading tensorboardx-2.6.4-py3-none-any.whl.metadata (6.2 kB)\n",
            "Collecting bitsandbytes<1.0,>=0.45.2 (from lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo)\n",
            "  Downloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl.metadata (10 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->torchgeo) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->torchgeo) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->torchgeo) (4.58.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->torchgeo) (1.4.8)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6->torchgeo) (3.2.3)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->torchgeo) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.5->torchgeo) (2025.2)\n",
            "Collecting affine (from rasterio!=1.4.0,!=1.4.1,!=1.4.2,>=1.3.3->torchgeo)\n",
            "  Downloading affine-2.4.0-py3-none-any.whl.metadata (4.0 kB)\n",
            "Requirement already satisfied: huggingface-hub>=0.24 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch>=0.3.3->torchgeo) (0.32.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.11/dist-packages (from segmentation-models-pytorch>=0.3.3->torchgeo) (0.5.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (3.1.6)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=2->torchgeo)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=2->torchgeo)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=2->torchgeo)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=2->torchgeo)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=2->torchgeo)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=2->torchgeo)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=2->torchgeo)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=2->torchgeo)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=2->torchgeo)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=2->torchgeo)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=2->torchgeo) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=2->torchgeo) (1.3.0)\n",
            "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (3.11.15)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24->segmentation-models-pytorch>=0.3.3->torchgeo) (1.1.2)\n",
            "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.11/dist-packages (from hydra-core>=1.0.0->lightly!=1.4.26,>=1.4.5->torchgeo) (4.9.3)\n",
            "Requirement already satisfied: docstring-parser>=0.15 in /usr/local/lib/python3.11/dist-packages (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (0.16)\n",
            "Collecting typeshed-client>=2.3.0 (from jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo)\n",
            "  Downloading typeshed_client-2.7.0-py3-none-any.whl.metadata (7.9 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (75.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.10.5->lightly!=1.4.26,>=1.4.5->torchgeo) (0.4.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.0->lightly!=1.4.26,>=1.4.5->torchgeo) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.27.0->lightly!=1.4.26,>=1.4.5->torchgeo) (3.10)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (2.19.1)\n",
            "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.11/dist-packages (from tensorboardX<3.0,>=2.2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (5.29.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=2->torchgeo) (3.0.2)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (1.3.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (1.6.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (6.4.4)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning!=2.3.*,!=2.5.0,>=2->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (1.20.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<14.0,>=12.3.0->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (0.1.2)\n",
            "Requirement already satisfied: importlib-resources>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from typeshed-client>=2.3.0->jsonargparse[signatures]<5.0,>=4.27.7; extra == \"pytorch-extra\"->lightning[pytorch-extra]!=2.3.*,!=2.5.0,>=2->torchgeo) (6.5.2)\n",
            "Downloading torchgeo-0.7.0-py3-none-any.whl (604 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m605.0/605.0 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading fiona-1.10.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m32.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia-0.8.1-py2.py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightly-1.5.21-py3-none-any.whl (855 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m855.8/855.8 kB\u001b[0m \u001b[31m61.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightning-2.5.1.post0-py3-none-any.whl (819 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m819.0/819.0 kB\u001b[0m \u001b[31m62.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rasterio-1.4.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (22.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.2/22.2 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading rtree-1.4.0-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (541 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m541.1/541.1 kB\u001b[0m \u001b[31m47.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading segmentation_models_pytorch-0.5.0-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m126.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m99.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m64.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m19.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m109.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchmetrics-1.7.3-py3-none-any.whl (962 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m962.6/962.6 kB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aenum-3.1.16-py3-none-any.whl (165 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading bitsandbytes-0.46.0-py3-none-manylinux_2_24_x86_64.whl (67.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.0/67.0 MB\u001b[0m \u001b[31m36.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click_plugins-1.1.1-py2.py3-none-any.whl (7.5 kB)\n",
            "Downloading cligj-0.7.2-py3-none-any.whl (7.1 kB)\n",
            "Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonargparse-4.40.0-py3-none-any.whl (224 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m224.3/224.3 kB\u001b[0m \u001b[31m24.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading kornia_rs-0.1.9-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.8/2.8 MB\u001b[0m \u001b[31m105.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lightly_utils-0.0.2-py3-none-any.whl (6.4 kB)\n",
            "Downloading lightning_utilities-0.14.3-py3-none-any.whl (28 kB)\n",
            "Downloading pytorch_lightning-2.5.1.post0-py3-none-any.whl (823 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.1/823.1 kB\u001b[0m \u001b[31m61.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tensorboardx-2.6.4-py3-none-any.whl (87 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.2/87.2 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading affine-2.4.0-py3-none-any.whl (15 kB)\n",
            "Downloading typeshed_client-2.7.0-py3-none-any.whl (624 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m49.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: aenum, typeshed-client, tensorboardX, rtree, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, lightning-utilities, lightly_utils, kornia_rs, jsonargparse, cligj, click-plugins, affine, rasterio, nvidia-cusparse-cu12, nvidia-cudnn-cu12, hydra-core, fiona, nvidia-cusolver-cu12, torchmetrics, kornia, bitsandbytes, pytorch_lightning, segmentation-models-pytorch, lightning, lightly, torchgeo\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[31mERROR: Operation cancelled by user\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cd /content/drive/MyDrive/Hopfield/"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5QUbAbA4jceH",
        "outputId": "d81473c5-5f76-408d-cfc1-212a7bbe77ce"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/drive/MyDrive/Hopfield\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "s5w_cJuPApV2"
      },
      "outputs": [],
      "source": [
        "# Importing necessary libraries\n",
        "import torch\n",
        "import numpy as np\n",
        "import time\n",
        "import pickle\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "from torch import nn\n",
        "\n",
        "# Store the appropriate device\n",
        "use_cuda = torch.cuda.is_available()\n",
        "use_mps = torch.backends.mps.is_available()\n",
        "\n",
        "if use_cuda:\n",
        "    device = torch.device(\"cuda\")\n",
        "elif use_mps:\n",
        "    device = torch.device(\"mps\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G35zcc_3Bcu7"
      },
      "source": [
        "# Training the HopfieldPooling\n",
        "\n",
        "In this section, we train the HopfieldPooling layer on the denoising task. To do so, we utilize the official implementation of the HopfieldPooling layer (https://github.com/ml-jku/hopfield-layers). As a result, it is necessary to clone this repository."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ngngwdACLNY",
        "outputId": "3da7b39c-369c-4cd0-ee84-438f7d0612e0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'hopfield-layers' already exists and is not an empty directory.\n",
            "Collecting git+https://github.com/ml-jku/hopfield-layers\n",
            "  Cloning https://github.com/ml-jku/hopfield-layers to /tmp/pip-req-build-y_jr2avu\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/ml-jku/hopfield-layers /tmp/pip-req-build-y_jr2avu\n",
            "  Resolved https://github.com/ml-jku/hopfield-layers to commit f56f929c95b77a070ae675ea4f56b6d54d36e730\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: torch>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from hopfield-layers==1.0.2) (2.6.0+cu124)\n",
            "Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from hopfield-layers==1.0.2) (2.0.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (4.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.5.0->hopfield-layers==1.0.2) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.5.0->hopfield-layers==1.0.2) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.5.0->hopfield-layers==1.0.2) (3.0.2)\n",
            "Building wheels for collected packages: hopfield-layers\n",
            "  Building wheel for hopfield-layers (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for hopfield-layers: filename=hopfield_layers-1.0.2-py3-none-any.whl size=25586 sha256=a2171d86347955c3c1803f2bcb480de1443d1ac0b03b9577313382b8d1df8096\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-j4p6ydx0/wheels/b1/d9/70/21518978ccebde4910a0ac6608e058be46118ad192d21f7fec\n",
            "Successfully built hopfield-layers\n",
            "Installing collected packages: hopfield-layers\n",
            "Successfully installed hopfield-layers-1.0.2\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/ml-jku/hopfield-layers.git\n",
        "!pip3 install git+https://github.com/ml-jku/hopfield-layers"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install light-the-torch >> /.tmp\n",
        "!ltt install torch torchvision >> /.tmp\n",
        "!pip install fastai --upgrade >> /.tmp"
      ],
      "metadata": {
        "id": "jVuvK81vv7Cb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "GSUaRwHtQw9L",
        "collapsed": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d988d25b-87fb-4e0a-aaa6-81d853d8da4b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/drive/MyDrive/Hopfield/train_denoising_task.py\", line 256, in <module>\n",
            "    main()\n",
            "  File \"/content/drive/MyDrive/Hopfield/train_denoising_task.py\", line 231, in main\n",
            "    train_set, val_set = train_test_split(train_data, test_size=0.2, stratify=train_data.targets)\n",
            "                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_param_validation.py\", line 216, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\", line 2876, in train_test_split\n",
            "    return list(\n",
            "           ^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/model_selection/_split.py\", line 2878, in <genexpr>\n",
            "    (_safe_indexing(a, train), _safe_indexing(a, test)) for a in arrays\n",
            "     ^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_indexing.py\", line 272, in _safe_indexing\n",
            "    return _list_indexing(X, indices, indices_dtype)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_indexing.py\", line 63, in _list_indexing\n",
            "    return [X[idx] for idx in key]\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/sklearn/utils/_indexing.py\", line 63, in <listcomp>\n",
            "    return [X[idx] for idx in key]\n",
            "            ~^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/torchvision/datasets/folder.py\", line 245, in __getitem__\n",
            "    sample = self.loader(path)\n",
            "             ^^^^^^^^^^^^^^^^^\n",
            "  File \"/content/drive/MyDrive/Hopfield/train_denoising_task.py\", line 150, in iloader\n",
            "    image = np.asarray((io.imread(path))/32000,dtype='float32')\n",
            "                        ^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/skimage/_shared/utils.py\", line 328, in fixed_func\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/skimage/io/_io.py\", line 82, in imread\n",
            "    img = call_plugin('imread', fname, plugin=plugin, **plugin_args)\n",
            "          ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/skimage/_shared/utils.py\", line 538, in wrapped\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/skimage/io/manage_plugins.py\", line 254, in call_plugin\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/skimage/io/_plugins/imageio_plugin.py\", line 11, in imread\n",
            "    out = np.asarray(imageio_imread(*args, **kwargs))\n",
            "                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/imageio/v3.py\", line 53, in imread\n",
            "    with imopen(uri, \"r\", **plugin_kwargs) as img_file:\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/imageio/core/imopen.py\", line 196, in imopen\n",
            "    plugin_instance = candidate_plugin(request, **kwargs)\n",
            "                      ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/imageio/plugins/pillow.py\", line 104, in __init__\n",
            "    with Image.open(request.get_file()):\n",
            "         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.11/dist-packages/PIL/Image.py\", line 3516, in open\n",
            "    prefix = fp.read(16)\n",
            "             ^^^^^^^^^^^\n",
            "KeyboardInterrupt\n"
          ]
        }
      ],
      "source": [
        "!python train_denoising_task.py --save-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GFnLqs8BS1vR",
        "outputId": "e0576e1c-f35c-4ffc-f65c-d92ca4a703c4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ],
      "source": [
        "# Loading the trained model\n",
        "from train_denoising_task import HopfieldModule\n",
        "hopfieldPooling = HopfieldModule()\n",
        "hopfieldPooling = hopfieldPooling.to(device)\n",
        "hopfieldPooling.load_state_dict(torch.load('models/hop.pt', map_location= device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p--jy8OxUNHb",
        "outputId": "b839572b-a76e-4c4f-ec1c-5b68b52d0e9f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.02460561736021191]"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ],
      "source": [
        "# Loading the training history\n",
        "with open('logs/hopfield_denoise.pkl', 'rb') as f:\n",
        "    hopfield_denoise_history = pickle.load(f)\n",
        "\n",
        "hopfield_denoise_history[\"loss\"]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fymnKxcs-dzI"
      },
      "source": [
        "# Training the base model\n",
        "\n",
        "To ensure repeatability, we train and use the default convolutional neural network provided by the official PyTorch repository:\n",
        "\n",
        "https://github.com/pytorch/examples/blob/main/mnist/main.py\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ldjibmsF-jKJ",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Training the baseline model\n",
        "!python conv_mnist.py --save-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "TpyvHxj7ATwy",
        "outputId": "619375ac-b508-4dc7-f70e-a7b83577489c"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-29-3988974454>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Loading the trained model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mconv_mnist\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mbaseline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mbaseline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'models/mnist_cnn.pt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_location\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_find_and_load_unlocked\u001b[0;34m(name, import_)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap.py\u001b[0m in \u001b[0;36m_load_unlocked\u001b[0;34m(spec)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mexec_module\u001b[0;34m(self, module)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_code\u001b[0;34m(self, fullname)\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.11/importlib/_bootstrap_external.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(self, path)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Loading the trained model\n",
        "from conv_mnist import Net\n",
        "baseline = Net()\n",
        "baseline = baseline.to(device)\n",
        "baseline.load_state_dict(torch.load('models/mnist_cnn.pt', map_location= device))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j4p0g9eKBVrD"
      },
      "source": [
        "# Loading MNIST-C Test Data\n",
        "\n",
        "In this section, we load and visualize the MNIST-C dataset. For this purpose, we used the implementation of `TORCH UNCERTAINTY` with some minor changes to fix some bugs.\n",
        "\n",
        "You can find their official repository here:\n",
        "\n",
        "https://github.com/ENSTA-U2IS-AI/torch-uncertainty/blob/main/torch_uncertainty/datasets/classification/mnist_c.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5UxXgOUBT2Ny"
      },
      "outputs": [],
      "source": [
        "import mnist_c\n",
        "\n",
        "# Loading data for all corruptions\n",
        "test_data_all = mnist_c.MNISTC(root=\".\", download=True, split = \"test\", transform=transforms.ToTensor(), subset = \"all\")\n",
        "\n",
        "# Create the test loader for all corruptions\n",
        "test_loader_all = torch.utils.data.DataLoader(test_data_all, batch_size=20, num_workers=1, shuffle = True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3Rm76jdOVLCk"
      },
      "outputs": [],
      "source": [
        "def visualize_data(data_loader) -> None:\n",
        "    \"\"\"\n",
        "    Helper method to visualize a sample of data.\n",
        "    :param data_loader: The data loader to pull the samples from.\n",
        "    :return: Nothing.\n",
        "    \"\"\"\n",
        "    # Create a plot for four random samples with their labels.\n",
        "    fig, ax = plt.subplots(2, 2, figsize=(6, 6))\n",
        "    # Get a random batch from the data loader.\n",
        "    images, labels = next(iter(data_loader))\n",
        "    # Display each image and label.\n",
        "    for i in range(4):\n",
        "        img = images[i].squeeze()\n",
        "        ax[i // 2, i % 2].imshow(img, cmap=\"gray\")\n",
        "        ax[i // 2, i % 2].axis(\"off\")\n",
        "        ax[i // 2, i % 2].set_title(f\"Label: {labels[i].item()}\")\n",
        "    # Check out the shape of one batch.\n",
        "    print(f\"Shape of a batch images: {images.shape}\")\n",
        "    print(f\"Shape of a batch labels: {labels.shape}\")\n",
        "\n",
        "visualize_data(test_loader_all)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "An5w-bEfVsPa"
      },
      "source": [
        "# The Integration Algorithm\n",
        "\n",
        "In this section, we implement our proposed integration algorithm using the pre-trained `hopfieldPooling` module and `baseline` model.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LijRuMNSWrTG"
      },
      "outputs": [],
      "source": [
        "mnistc_subsets = [\n",
        "    \"identity\",\n",
        "    \"brightness\",\n",
        "    \"canny_edges\",\n",
        "    \"dotted_line\",\n",
        "    \"fog\",\n",
        "    \"glass_blur\",\n",
        "    \"impulse_noise\",\n",
        "    \"motion_blur\",\n",
        "    \"rotate\",\n",
        "    \"scale\",\n",
        "    \"shear\",\n",
        "    \"shot_noise\",\n",
        "    \"spatter\",\n",
        "    \"stripe\",\n",
        "    \"translate\",\n",
        "    \"zigzag\",\n",
        "]\n",
        "\n",
        "def test_hop(\n",
        "    basemodel: nn.Module, hop: None | nn.Module, cdae: None | nn.Module, corruption: str\n",
        ") -> None:\n",
        "    test_data = mnist_c.MNISTC(root=\".\", split = \"test\", transform=transforms.ToTensor(), subset = corruption)\n",
        "    test_loader = torch.utils.data.DataLoader(test_data, batch_size=20, num_workers=1, shuffle = True)\n",
        "    number_use = 0\n",
        "    basemodel.eval()\n",
        "    test_loss = 0\n",
        "    correct = 0\n",
        "    number_hop_use = 0\n",
        "    added_time = 0\n",
        "    condition = hop is not None or cdae is not None\n",
        "    with torch.no_grad():\n",
        "        for data, target in test_loader:\n",
        "            data, target = data.to(device), target.to(device)\n",
        "            if condition:\n",
        "\n",
        "                output1 = basemodel(data)\n",
        "\n",
        "                if hop is not None:\n",
        "                    output2 = basemodel(hop(data))\n",
        "                else:\n",
        "                    output2 = basemodel(cdae(data))\n",
        "\n",
        "                prob1, pred1 = output1.max(\n",
        "                    dim=1, keepdim=True\n",
        "                )  # get the index of the max log-probability\n",
        "                prob2, pred2 = output2.max(dim=1, keepdim=True)\n",
        "                mult = prob1 > prob2\n",
        "                pred = (mult * pred1) + ((~mult) * pred2)\n",
        "                number_use += (~mult).sum().item()\n",
        "\n",
        "            else:\n",
        "                output = basemodel(data)\n",
        "                pred = output.argmax(\n",
        "                    dim=1, keepdim=True\n",
        "                )  # get the index of the max log-probability\n",
        "\n",
        "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "\n",
        "    test_loss /= len(test_loader.dataset)\n",
        "\n",
        "    print(\n",
        "        \"\\nCorruption: {}, {}{} -> Accuracy: {}/{} ({:.2f}%)\\n\".format(\n",
        "            corruption,\n",
        "            (\n",
        "                (\", Hopfield-integrated\" if hop is not None else \"CDAE-integrated\")\n",
        "                if condition\n",
        "                else \"not-integrated\"\n",
        "            ),\n",
        "            \", Percentage of Use: ({}/{}:{:0.2f})\".format(number_use,\n",
        "                                                        len(test_loader.dataset),\n",
        "                                                        number_use / len(test_loader.dataset) * 100)\n",
        "            if condition\n",
        "            else \"\",\n",
        "            correct,\n",
        "            len(test_loader.dataset),\n",
        "            100.0 * correct / len(test_loader.dataset),\n",
        "        )\n",
        "    )\n",
        "\n",
        "    if condition:\n",
        "        return correct / len(test_loader.dataset), number_use / len(test_loader.dataset)\n",
        "    else:\n",
        "        return correct / len(test_loader.dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8K7g_IIcAnA"
      },
      "outputs": [],
      "source": [
        "acc, acc_hop, hop_usage = {}, {}, {}\n",
        "\n",
        "for sub in mnistc_subsets:\n",
        "  acc[sub] = test_hop(baseline, hop = None, cdae= None, corruption=sub)\n",
        "  acc_hop[sub], hop_usage[sub] = test_hop(baseline, hop = hopfieldPooling, cdae= None, corruption=sub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "whtDsW33dIYN"
      },
      "outputs": [],
      "source": [
        "# Save logs\n",
        "with open(f'logs/acc.pkl', 'wb') as f:\n",
        "  pickle.dump(acc, f)\n",
        "\n",
        "with open(f'logs/acc_hop.pkl', 'wb') as f:\n",
        "  pickle.dump(acc_hop, f)\n",
        "\n",
        "with open(f'logs/hop_usage.pkl', 'wb') as f:\n",
        "  pickle.dump(hop_usage, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PHSicgNPfcK3"
      },
      "outputs": [],
      "source": [
        "# Calculate corruption robustness metrics\n",
        "def calculate_robustness_metrics(acc, acc_integrated):\n",
        "  baseline = {\"corruption_accuracy\": np.mean(np.array([v*100 for k, v in acc.items() if k != \"identity\"])),\n",
        "              \"relative mCE\": 100,\n",
        "              \"mCE\": 100}\n",
        "  integrated = {\"corruption_accuracy\": np.mean(np.array([v*100 for k, v in acc_integrated.items() if k != \"identity\"])),\n",
        "              \"relative mCE\": None,\n",
        "              \"mCE\": None}\n",
        "\n",
        "\n",
        "  def mCE(relative = False):\n",
        "    numerator = 0\n",
        "    denominator = 0\n",
        "    for c in acc.keys():\n",
        "      if c != \"identity\":\n",
        "        denominator += (1-acc[c]) - ((1-acc[\"identity\"]) if relative else 0)\n",
        "\n",
        "    for c in acc_integrated.keys():\n",
        "      if c != \"identity\":\n",
        "        numerator += (1-acc_integrated[c]) - ((1-acc_integrated[\"identity\"]) if relative else 0)\n",
        "\n",
        "    return numerator/denominator*100\n",
        "\n",
        "  integrated[\"relative mCE\"] = mCE(relative = True)\n",
        "  integrated[\"mCE\"] = mCE(relative = False)\n",
        "\n",
        "  return baseline, integrated"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IevTPvnLhVxA"
      },
      "outputs": [],
      "source": [
        "baseline_metrics, hopfield_integration_metrics = calculate_robustness_metrics(acc, acc_hop)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XmDGwPylhhjT"
      },
      "outputs": [],
      "source": [
        "baseline_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01A1W0W4hiuX"
      },
      "outputs": [],
      "source": [
        "hopfield_integration_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wIlWmTLohkny"
      },
      "source": [
        "# Ablation Study\n",
        "\n",
        "In this part, we replace the HopfieldPooling layer with a stacked Convolutional Denoising Autoencoder (CDAE) and compare the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6mAVH4Ssk48f"
      },
      "source": [
        "### Denoising Task"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wK9G-K-Qh-ny"
      },
      "outputs": [],
      "source": [
        "!python train_denoising_task.py --cdae --save-model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7dnQciNeiMlH"
      },
      "outputs": [],
      "source": [
        "# Loading the trained model\n",
        "from train_denoising_task import CDAE\n",
        "cdae = CDAE()\n",
        "cdae = cdae.to(device)\n",
        "cdae.load_state_dict(torch.load('models/CDAE.pt', map_location= device))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "go6cqPHfkLB2"
      },
      "outputs": [],
      "source": [
        "# Loading the training history\n",
        "with open('logs/CDAE_denoise.pkl', 'rb') as f:\n",
        "    cdae_denoise_history = pickle.load(f)\n",
        "\n",
        "cdae_denoise_history[\"loss\"]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KaAad3hjJsbn",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "# Compare HopfieldPooling layer and CDAE in terms of MSE for the denoising task\n",
        "plt.figure(figsize=(15, 5), dpi = 200)\n",
        "plt.plot(hopfield_denoise_history[\"loss\"], marker = \"o\", label = \"Hopfield\")\n",
        "plt.plot(cdae_denoise_history[\"loss\"], marker = \"^\", label = \"Autoencoder\")\n",
        "plt.xticks(range(20), labels=[f\"{i}\" for i in range(1, 21)])\n",
        "plt.legend()\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"MSE\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T0mBYkaXkuEU"
      },
      "source": [
        "### Integration Algorithm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k_uAomPwlAtB",
        "collapsed": true
      },
      "outputs": [],
      "source": [
        "acc_AE, AE_usage = {}, {}\n",
        "\n",
        "for sub in mnistc_subsets:\n",
        "  acc_AE[sub], AE_usage[sub] = test_hop(baseline, hop = None, cdae= cdae, corruption=sub)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FzLFyiFnm3aB"
      },
      "outputs": [],
      "source": [
        "with open(f'acc_AE.pkl', 'wb') as f:\n",
        "  pickle.dump(acc_AE, f)\n",
        "\n",
        "with open(f'AE_usage.pkl', 'wb') as f:\n",
        "  pickle.dump(AE_usage, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ATQYHItLlR0D"
      },
      "outputs": [],
      "source": [
        "baseline_metrics, cdae_integration_metrics = calculate_robustness_metrics(acc, acc_AE)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8uX7por3mOKW"
      },
      "outputs": [],
      "source": [
        "baseline_metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nDqlT2cKmoWX"
      },
      "outputs": [],
      "source": [
        "cdae_integration_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hVjjZ4PUqVSN"
      },
      "source": [
        "### Visualize Output of Modules"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sGhrKnDcqeEC"
      },
      "outputs": [],
      "source": [
        "batch_size = 20\n",
        "corruption = \"fog\"\n",
        "\n",
        "test_data = mnist_c.MNISTC(root=\".\", split = \"test\", transform=transforms.ToTensor(), subset = corruption)\n",
        "test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, num_workers=1, shuffle = True)\n",
        "\n",
        "# obtain one batch of test images\n",
        "dataiter = iter(test_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "# get sample outputs\n",
        "output1 = hopfieldPooling(images)\n",
        "output2 = cdae(images)\n",
        "\n",
        "# output is resized into a batch of images\n",
        "output1 = output1.view(batch_size, 1, 28, 28)\n",
        "output2 = output2.view(batch_size, 1, 28, 28)\n",
        "# use detach when it's an output that requires_grad\n",
        "output1 = output1.cpu().detach().numpy()\n",
        "output2 = output2.cpu().detach().numpy()\n",
        "\n",
        "# plot the first seven input images and then reconstructed images\n",
        "fig, axes = plt.subplots(nrows=3, ncols=7, sharex=True, sharey=True, figsize=(10,4), dpi = 200)\n",
        "\n",
        "r, c = 0, 0\n",
        "y_labels = [\"Corrupted\", \"Hopfield\", \"Autoencoder\"]\n",
        "# input images on top row, reconstructions on bottom\n",
        "for img, row in zip([images.cpu(), output1, output2], axes):\n",
        "    c = 0\n",
        "    for i, (img, ax) in enumerate(zip(img, row)):\n",
        "        if not r:\n",
        "           ax.set_title(f\"True label: {labels[i].item()}\")\n",
        "        ax.imshow(np.squeeze(img), cmap='gray')\n",
        "        ax.get_xaxis().set_visible(False)\n",
        "        ax.set_yticks([])\n",
        "        if not c:\n",
        "          ax.set_ylabel(y_labels[r])\n",
        "        c+=1\n",
        "    r+=1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CTmNDXFsMt0Q"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}